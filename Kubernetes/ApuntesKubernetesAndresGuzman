KUBERNETES

Importante: cosas como comentarios y explicaciones de archivos yaml estan en esta ruta: C:\Users\andre\PROCEDIMIENTOS\Kubernetes o C:\Users\andre\PROCEDIMIENTOS\Kubernetes\2_curso-kubernetes[secret]\curso-kubernetes
Sin embargo, el proyecto como tal (codigo) se sigue modificando de lo q venia en Docker, o sea aca: C:\Users\andre\Desarrollo\Curso_Microservicios_Docker_Kubernates_AWS. 
En estas rutas estan los archivos con comentarios, sin embargo, si queremos copiar para pegar en otro proyecto, mejor descargar de los archivos del curso. 

Podemos darle instrucciones a nuestro Cluster por medio de un cliente externo que se está comunicando
El cliente envía instrucciones al cluster de kubernetes, el que por medio de la apiserver que se encuentra dentro del masternode se encarga de ejecutar adecuademente estas instrucciones, ya sea crear un pod, crear un deployment, etc.
El Scheduler, que se encuentra dentro del masternode, se encarga de ver la disponibilidad de recursos de las maquinas, asignando el recurso recien creado (pod, deployment, etc.) a la máquina mas liviana.
En el master node hay un componente llamado cloud controller manager manda una instruccion al cloud provider api (de AWS, por ejemplo) para q cree un load balancer.
Como clientes podemos comunicarnos con el Cluster de forma imperativa (con lineas de comandos de kubetcl en local) o bien declarativa por medio de un archivo yaml que contenga la configuracion de lo que deseamos hacer en el Cluster.
El pod es un objeto de Kubernetes. Cada pod puede contener varios contenedores, pero lo usual es que sea solo uno. En el fondo es sólo una cáscara que envuelve al contenedor.
Los contenedores de un pod se comunican entre ellos por localhost.
Los pod se comunican entre si por IP adress (TODO: ¿pero para los pods de un mismo worker node o de diferentes worker nodes? En local con minikube no interesa tanto, pero en despliegue en produccion sí) , pero dado que los pods son altamente volátiles y desechables, no es recomendable unirlos de esta forma, sino por el IP estático de otro objeto como el service.
Los pods tb pueden tener BBDD por medio de volúmenes, pero ya q los pod son desechables, se recomienda guardar BBDD en el cluster.

El archivo deployment crea y administra los pods por nosotros. Es un archivo yaml con la configuracion de memoria, volumenes, etc. para nuestros pods. Lo mas importante es que contiene el estado deseado de despliegue para administrar los pods y los crea por nosotros.
El service crea y administra una red por nosotros. Por medio del nombre o el IP de esta red se comunican los pods entre ellos.

minikube es una herramienta que permite virtualizar kubernetes, pero solo para trabajar en nuestro local.
kubectl es una herramienta que permite simular un cliente que está ejecutando kubernetes (en local kubectl se comunica con minukube).

Nota: Los comandos están en archivo "ComandosMiniKubeYKubectl".
Nota: Revisar los comentarios para los archivos yaml. Los comentarios están repartidos entre todos estos archivos, ya q se fue tomando nota a partir de lo que Guzman decía, por lo que se recomienda revisarlos todos

Lo que él hizo en local fue:
1. Levantó el cluster virtual con minikube.
2. Por medio del cliente kubectl creó imperativamente un deployment con la imagen mysql8 desde docker hub. Esto indirectamente generó ademas un pod. Notar que al parecer no existe un comando para crear pods directamente, sino q se hace a través de la creacion de un deployment.
3. En el paso anterior se dió cuenta de que no se puede agragar variables de ambiente por comandos, por lo que mejor generó un archivo yaml de deployment (ver comandos) y le agregó las variables de ambiente (clave valor para password y database). Luego ejecutó este archivo yaml para crear un deployment declarativamente. 
4. Creó un servicio al exponer el deployment de mysql8 (ver comandos), con el fin de obtener un nombre de dominio fijo y IP fija y de esta manera poder conectar internamente los pod de mysql8 y msvc-usuarios, este último a ser creado aun.
5. Generó un deployment para msvc-usuarios, lo que indirectamente creó un pod. Lo hizo imperativamente, ya que acá sí tenía las variables de ambiente.
6. Expuso el deployment de msvc-usuarios (=generó un servicio), pero no para comunicacion interna como en el caso anterior, sino para comunicación externa con kubectl expose deployment msvc-usuarios --port=8001 --type=LoadBalancer
Notar en este punto que un servicio lo podemos crear con varias opciones (ver comandos) como con Load Balancer, etc.
7. Obtuvo la IP externa de msvc-usuarios en minikube (ver comandos) y probó en postman con esta ip y el puerto externo con GET http://ipMinikube:30507. Obtuvo un array de usuarios vacío, que debemos poblar mandando POST a http://ipMinikube:30507 con datos de usuario.
8. Actualizó la imagen de un deployment. 
Para ello en la clase UsuarioController, con el objetivo de generar un crash en la aplicación, agregó:
@GetMapping("/crash")
public void crash() {
((ConfigurableApplicationContext)context).close();
} 
A continuación construyó la imagen docker con este cambio y subió la imagen a docker hub.
Luego en el pod verificó el nombre del container que estaba usando y vió que no era la misma que acababa de subir a dockerhub. Por ello actualizó la imagen imperativamente (ver comandos).
Luego probó en postman con http:172.3187477474:30507/crash y vio que se le reinició la aplicacion debido al crash ya que con comando kubectl get pods aparece RESTARTS en 1 y no en 0. Ojo, en el pod en status debe aparecer como Running, no como Completed para q este levantado (se puede demorar un poco y aparecer como Completed primero).
Finalmente se creó una nueva imagen (o sea v3) eliminando los 20 seg en linea CMD del dockerfile de msvc-usuarios y volvió a revisar los pods. 
9.  Escaló el deployment msvc-usuarios con tres replicas, con ello generamos tres instancias, o sea tres pods con carga balanceada.
10. Probó en postman con http:172.3187477474:30507/crash y con kubectl get pods vió que un pod tenía un restart de 1 , pero los otros 2 tienen restarts de 0 y status de Running, con lo q se comprueba que la carga está balanceada.
11. Volvió a escalar el deployment, pero solo con una réplica.
12. Generó un nuevo archivo yaml para el servicio de mysql8. (ver archivo de nombre svc-mysql.yaml) Este lo editó, ya q borró metadata como fecha de creación y otros datos. En spec solo dejó los puertos. Port es el puerto externo y targetPort es el interno del contenedor (para nuesto caso en ambos es 3306).
En este punto tiene dos archivos yaml: uno para el deployment llamado deployment-mysql.yaml y otro para el servicio llamado svc-mysql.yaml
13. Generó un nuevo archivo llamado svc-usuarios.yaml para msvc-usuarios (lo mismo del paso anterior pero para msvc-usuarios). ¿¿¿Anteriormente tenía el deployment-usuarios.yaml ???-->Al parecer, sí.
14. Generó un nuevo deployment a partir del archivo deployment-usuarios.yaml, con tres replicas. En este punto entonces tiene dos deployment: uno para mysql8 y otro para msvc-usuarios.
15. Verificó (con kubectl describe service msvc-usuarios) que en el selector aparece Selector: app=msvc-usuarios, ya q con esta etiqueta q escribimos en el yaml va a seleccionar los pods q contengan esa etiqueta, que en este caso fueron las tres réplicas o tres pods que creamos con el deployment de msvc-usuarios. Los endpoints de cada pod, con IP:puerto interno tambien lo podemos ver acá.
Estos pods son volátiles, por lo que mejor usamos la IP del servicio o el Name del servicio para comunicacion interna entre los pods.
16. Explicó que existe en minikube la interfaz gráfica dashboard. Esto es super util ya q ahorra tiempo. Se explica en el video titulado: Trabajando con la forma Declarativa yaml. Se activa con minikube dashboard.
17. Creó el archivo deployment-postgres.yaml (ver archivo) para hacer el deployment de postgres, lo q indirectamente generará un pod (solo uno, ya que no le puso replicas en el yaml)
18. Creó el archivo svc-postgres.yaml (ver archivo) para hacer el service de postgres.
19. Ejecutó ambos archivos anteriores(con comando apply, ver comandos). Luego de ejecutar tiene 5 pods es este punto: los tres de msvc-usuarios (creados con las replicas), uno de mysql y uno de postgress.
20. Escribió los archivos deployment-cursos.yaml y svc-cursos.yaml y los ejecutó (con comando apply). Obtuvo el puerto externo en forma análoga a como lo hizo para msvc-usuarios, pero esta vez para cursos (comando kubectl get services)
21. Para msvc-cursos obtvo la IP de minikube (comando minikube service msvc-cursos --url) y probó en postman con ruta GET Pminikube/puertoExterno
22. Luego de comprobar q funcionaba, en el mismo postman asignó usuarios a los cursos con la ruta PUT IPminikube/asignar-usuario/1 (1 porq es al curso numero 1).



VOLUMENES

Las BBDD si no se crean volúmenes  se encuentran en los containers. Sucede que muchas veces producto de trafico excesivo o de autoescalamiento el o los containers de un pod se reinician, perdiendose estos datos.
Es posible en Kubernetes tambien guardar volumenes en los pods (ver pdf de guzman), sin embargo si el pod se elimina tambien se pierde la informacion.
Es por ello que debemos crear volumenes que pertenezacan al worker node (o sae a la maquina) y no al container ni al pod, para persistir datos y que se puedan compartir siempre entre containers pertenecientes a la misma maquina.
Para ello requerimos un driver. Existen varios driver (driver=Types of Volumes) dependiendo del proveedor (aws, azure), etc, del numero de nodos y de otros factores. Uno de estos driver es CSI, que es una especificación que debe ser implementada por algun proveedor (como Hibernate implementa JPA) como AWS. CSI trabaja con varios nodos y por lo mismo suele ser ocupado en produccion
Para nuestro caso sin embargo que trabajamos com minikube con un solo nodo, usaremos el driver hostPath, q sirve para un nodo unico. 

Para la siguiente configuración, recordar q aunque el volumen está fuera del pod pero dentro del nodo debido al uso de hostPath, la configuración se hace en el deployment. 

1. Modificó el archivo deployment-mysql.yaml (ver archivo) incluyendo la configuracion de volumenes.
2. Con comando minikube service msvc-usuarios --url  y despues minikube service msvc-cursos --url verificó IP y puertos para probar en postman con ip:puerto con los GET y los POST
3. Ejecutó el deployment-mysql.yaml con comando apply (esto reinicia los containers, por lo que solo por esta vez el GET para usuarios estará vacío ). Para comprobarlo vió el log del pod con kubectl logs claveDelPod y se fijó q habia un error.
4. Con comando kubectl delete -f.\deployment-mysql.yaml borró el deployment (con los pods) debido a la situacion del punto anterior y luego lo creó de nuevo con comando apply. Comprobó q ahora si se estan guardando los datos aun si bota el pod debido al volumen q está fuera de este.
5. Realizó los mismos cuatro pasos anteriores pero para la BBDD de Postgres con el archivo deployment-postgres.yaml (no esta en el archivo mio porque es lo mismo y me dio lata, pero se puede descargar al final de la seccion)

Aparte de lo anterior existe la posibilidad de tener una configuración para volumenes que pertenezca al cluster y que no esté anclado a un despliegue en particular, por lo q el volumen no va a estar anclado a un pod o a un nodo. Esto se logra con
Persistant Volumes, en donde se tiene un archivo yaml con configuracion general, que podrá ser ocupado por uno o varios nodos o pods. En el yaml individual del pod debemos llamar a este archivo general a través de claims para tener acceso a los recursos guardados en el cluster.
Para pv:
1. Escribir el archivo de persistent volume (ver archivo mysql-pv.yaml).
2. Escribir el archivo de persistent volume claims para los pods (ver archivo mysql-pvc.yaml).
3. Modificar la configuracion de los pods incluyendo persistent volume claims en archivo deployment (ver archivo deployment-mysql.pvc.yaml).


CONFIGMAP Y SECRET

Nota:antes q nada en deployment-usuarios bajó el numero de replicas de tres a uno.
Configmap es una configuracion centralizada en donde el boilerplate q se repite en todos los deployments (en este caso nuestros deployments para usuarios y cursos) estan en un mismo archivo. Para el configmap, los puertos host y database los saca de application.properties, como siempre
En los cuatro archivos yaml de deployments, debe hacerse referencia a las variables que seteamos en el configMap con valueFrom: y configMapKeyRef: (no lo puse en los archivos, pero se pueden ver al descargar los archivos del curso).
El archivo secret.yaml permite codificar en base 64 la informacion sensible, como username (root) y password (sasa). En los archivos de deployment debe hacerse referencia al secret.yaml por medio de valueFrom: secretKeyRef:
1. Escribió el configMap.
2. Modificó los cuatro deployments
3. Ejecutó el configMap y los 4 deployments (con comando appy).
4. Escribió el secret.yaml y en deployment-cursos y deployment-usuarios extrajo la info sensible desde el secret.
5. En el configmap comentó la info sensible, porque los deployments lo estan obteniendo del secret.
6. Desplegó todo de nuevo (incluyendo el secret, con comando apply como siempre) y volvio a probar en postman.

SPRING CLOUD KUBERNETES

Modificó los proyectos para q se pudieran comunicar mediante spring cloud kubernetes.
Para ello en ambos proyectos:
1. Agregó las tres dependencias de kubernetes.
2. Eliminó las url en las interfaz UsuarioClientRest y CursoClientRest, ya q en kubernetes la comunicacion es solo mediante nombre de servicio (que se encuentra en application.properties)
3. En ambas clases main agregó la configuracion @EnableDiscoveryClient //para poder usar spring cloud kubernetes
4. Generó las imagenes para ambos servicios en docker y las subió a docker hub.
5. Borró los deployment de msvc-cursos y msvc-usuarios, quedandose momentaneamente solo con los deployment de mysql y postgress.
6. Ejecutó los deployments borrados en el paso anterior (comando apply), para que tomara los cambios hechos a la imagen de dockerhub en pasos anteriores.
7. Pese a que los pods se muestran como running, con comando logs cachó que hay errores. Esto es porque los microservicios no se están comunicando al api de kubernetes. Para ello debemos dar privilegios de administrador (ver comandos).
8. Creó el priviliegio de administrador de cluster y ya los errores desaparecieron de los logs. Muestra un mensaje q la carga está balanceada para los pods via spring cloud debido a la configuracion q hicimos en el proyecto. Muestra tambien en los logs un mensaje de "discovery client is now available", lo q significa el microservicio (msvc-usuarios, por ejemplo)  se puede comunicar con los otros microservicios (msvc-cursos, por ejemplo). Muestra otro mensaje For 'msvc-usuarios' URL not provided. Will try picking an instance via load-balancing, o sea va a ocupar la configuracion de spring que realizamos en pasos anteriorespara balancear la carga si tuviera mas de una instancia.
9. Probó igual q anteriormente en postman para obtener cursoUsuarios con GET http://172.18.2013.250:32701 y vio q le funcionaba ok, pero esta vez es con la configuracion de spring cloud kubernetes q ocupa solo el nombre del servicio, prescindiendo de las url

Se pueden setear variables de ambiente para ver informacion en forma visual acerca del balanceo de carga entre los pods en el postman. Para ello, solo en msvc-usuarios realizó lo siguiente:
1. Agregó variables de ambiente en el archivo de deployment.yaml para ver la variable deseada segun la nomenclatura que esta estipulada en la documentacion de kubernetes. Para ver el nombre del pod e ip del pod (ver archivo q dice deployment-usuarios-seccion-spring.xml)
2. Incorporó estas variables en clase UsuarioController
3. Cambió el numero de réplicas a tres en msvc-usuarios. Desplegó nuevamente con comando apply
3. Probó en postman deshabilitando la opcion Connection en la cabecera. Probó varias veces con el get y vio que unas veces tomaba un nombre e ip de pod y otras veces cambiaba.

Tambien es posible setear las configuraciones del application properties fuera del proyecto de spring, en el configMap de Kubernetes, de esta forma si hay algun cambio de configuracion no hay q estar subiendo a dockerhub y haciendo todo el proceso de nuevo.
Para ello: 
1. Modificar el configMap.yaml agregando la configuracion application.yaml
2. Modificar UsuarioController (solo lo probó con msvc-usuarios) con clave valor a ser leído como texto en Postman
3. Notar que agregaríamos normalmente algo en application.properties para q el texto del punto anterior fuera leido; acá sin embargo no seteamos nada en el application.properties
4. Subir la imagen de nuevo a dockerhub. Borrar el deployment-usuarios y levantarlo de nuevo. Levantar tambien el configMap (comando apply)
5. Probar en postman con GET http://172.18.2013.250:32701

Las configuraciones del application properties en kubernetes de arriba pueden ser subdivididas en configuraciones para desarrollo y produccion. Para ello:
1. Setear en el application properties del proyecto (solo lo hizo con msvc-usuarios de nuevo) la configuracion para desarrollo o produccion con spring.profiles.active=dev o spring.profiles.active=prod (solo lo hizo con dev).
2. Escribir la configuracion en el configMap para dev y/o prod. No hizo nada en UsuarioController porque ya estaba agregado del exto anterior el put para que tomara cualquier texto q le pongamos en configMap.
3. Subir la imagen de nuevo a dockerhub. Borrar el deployment-usuarios y levantarlo de nuevo. Levantar tambien el configMap (comando apply)
4. Probar en postman con GET http://172.18.2013.250:32701

Existen dos tipos de pruebas en Kubernetes para analizar la salud de nuestros pods y contenedores.
-Liveness: que se encarga de ver cada cierto lapso de tiempo que la aplicacion no se haya caido (q no dé un estatus 500). Si se ha caído sencillamente reinicia el contenedor en el pod (no el pod, que se mantiene tal cual).
-Readiness: que se encarga de verificar si los servicios de nuestros pods estan en condiciones de recibir peticiones http adecuademente. Informa si un pod/servicio está muy sobresaturado y ya no es capaz de recibir peticiones http por lo que está derivando las peticiones a otras instancias (pods) hasta que baje el trafico y vuelva a estar operativo. 
El kublet (perteneciente a cada worker node, ver diapos) hace esto en forma automatica por defecto en kubernetes, pero la idea de esta seccion es configurar ciertos parametros para tener un control personalizado de liveness y readiness.  
Por ejemplo si existe un pod que no esta funcionando bien, las peticiones se derivarán a otros pods. Solo para msvc-usuarios realizó lo siguiente:
1. Agregar en pom dependencia de spring boot actuator, que permite ademas chequear otras cosas interesantes ademas de readiness y liveness como metricas, chequeos de url, etc.
2. En application properties escribir la configuracion para readiness y liveness. En archivo deployment (en este caso deployment-usuarios-seccion-spring) escribir variables de ambiente para readiness y liveness.
3. Subir la imagen de nuevo a dockerhub. Ejecutar el archivo de deployment (deployment-usuarios-seccion-spring en nuestro caso).
4. Probar en postman. Primero igual que en los casos anteriores (para confirmar q funciona). Luego con un GET probar para msvc-usuarios con GET http://172.18.2013.250:31320/actuator luego http://172.18.2013.250:31320/actuator/info y luego http://172.18.2013.250:31320/actuator/health y luego http://172.18.2013.250:31320/actuator/env y luego con http://172.18.2013.250:31320/actuator/health/readiness y luego http://172.18.2013.250:31320/actuator/health/liveness

Kubernetes permite personalizar la asignacion de recursos a nuestros containers (si no pernosalizamos igual esto se realiza por defecto), como memoria ram y/o cpu. Esto al igual q los 2 temas anteriores tambien se setea en las variables de entorno del archivo deployment (en nuestro caso deployment-usuarios-seccion-spring). Naturalmente los limits van a depender de la disponibilidad de recursos q tengamos en nuestro worker node o machine en el proveedor (aws, por ejemplo).
 Solo para msvc-usuarios realizó lo siguiente:
1. Agregó requests y limits en deployment-usuarios-seccion-spring
2. Borró y ejecutó (comandos apply y delete) deployment-usuarios-seccion-spring. Se demora al levantar!!

GATEWAY

Gateway es un servidor de enrutamiento dinamico compuesto por filtros de funcionalidad especifica autorizacion, seguridad, analisis estadisticos, cors etc, de forma de manejar todo esto en forma centralizada en todo el ecosistema y no individualmente para cada microservicio. Ya vienen filtros por defecto para enrutar, pero podemos tb personalizadar estos filtros para implementar multilenguajes, manejos de error, modificar cabeceras o parametros del request, manejar respuesttas estaticas para ciertas rutas, interceptar rutas a un servicio en particular y modificar su enrutamiento a otro servicio, etc,
Existen dos implementaciones: Zuul de Netflix y Spring Cloud Gateway (con comunicacion asincrona).
El gateway es el punto de entrada al ecosistema de microservicios desde el exterior (internet) y sera el encargado de enrutar cada uno de estos servicios con url individuales a una url base. 

1. Creó el proyecto msvc-gateaway como modulo hijo de curso-kubernetes. Luego agregó dos de las dependencias de Kubernetes.
2. Creó el archivo application.yaml en resources del proyecto msvc-gateaway, para enrutar msvc-cursos y msvc-usuarios individualmente a esta configuracion centralizada que es el gateway; ademas en este archivo hay que indicar que vamos a proveer loadbalancer (de spring cloud en nuestro caso). La configuraciones de este archivo application.yaml tambien se podrian haber hecho en el application.properties nornal con la configuracion que sale en el curso
3. Se creó un dockerfile para msvc-gateaway, construyó la imagen, le puso un tag (siempre le puso un tag en todos los casos anteriores, por si acaso) y la subió a  dockerhub. 
4. Creó el archivo gateway.yaml, donde colocó juntos el deployment (con la imagen del dockerfile q creó en el paso anterior) y el servicio del gateway. No lo tengo como archivo, porque me dio lata ya q es repetir lo hecho para msvc-cursos o msvc-usuarios, pero se puede descargar de los archivos del curso.
5. Verificó el deployment (con kubectl get pods) y el servicio (con kubectl get services). En el servicio cachó que tiene asignado el puerto 8090:30074/TCP para gateway (el 30074 asignado automaticamente por minikube).
6. Con comando minikube service msvc-gateaway --url cachó la ruta http://172.29.201.32:30074 para probar en postman
7. En postman probó con GET http://172.29.201.32:30074/api/usuarios (recordar q /api/usuarios fue la ruta q se le dió a msvc-usuarios en archivo application.yaml). Notar que es el loadbalancer de spring cloud el q esta en uso, no el de kubernetes, ya que aunque tiene el connection en los headers, le funciona impeca.


EKS de AWS

AWS EKS= Elastic Kubernetes Service administrado por Amazon. Es para trabajar en el mundo real con varios nodos (nodos=maquinas), en vez de un solo nodo q es lo q hemos venido haciendo con minikube en local. Este mismo servicio lo tiene GCP, Digital Ocean y Azure y otros proveedores.
Anteriormente se vio ECS para trabajar con contenedores en AWS, en esta sección en cambio de trabajará con nodos en AWS.
Tambien podriamos contratar un VPS en cualquier proveedor de VPS (incluso en amazon) e ir instalando uno a uno los diferentes software/componentes de kubernetes como kubelet, kube proxy, etc. pero es mucho mas lento y complejo. Con EKS es mar rapido y automatico.

1. Ir a la pagina de EKS en AWS. https://us-east-1.console.aws.amazon.com/eks/home?region=us-east-1#  (ojo q no es gratuito esto!)
2. Crear Cluster en aws.
Hacer click en "crear cluster". Entonces va a aparecer la pantalla de "configurar cluster" (ver pantallazos). Dar nombre, version y seleccionar rol.
Si no se tiene un rol creado previamente, hacer click en "Guia usuario de Amazon".
Despues va a pedir "especificar redes", pero nosotros estamos creando una red para comunicacion interna entre los pods y externa desde internet, por lo que debemos crear una pila, que es lo que nos proveerá de un conjunto de recursos de red para la VPC, como la red, subredes o subnet, internet gateway, natgateway, grupos de seguridad, etc.
Para ello escribir cloudformation en el buscador de aws y llegar con los pantallazos hasta "crear pila". En la pantalla de crear pila va a pedir la URL de amazon s3, que la obtenemos de la documentacion (pantallazos).
Luego en pantalla de especificar detalles de la pila damos un nombre y dejamos todo lo demas por defecto y creamos la pila.
Nos vamos a CloudFormation-->Pilas y revisamos q la pila se haya terminado de crear (tarda un poco). Esto nos proveerá de un VPC (virtual private cloud) que es una red virtual aislada, con subnet privada para comunicacion interna entre nuestros pods y publica para comunicación con internet.
Luego vuelve a "especificar redes" y selecciona la pila recien creada. Cons la sola excepcion de donde dice "acceso al punto de enlace del cluster" que se debe dejar en publico y privado, todo lo demas es por defecto y creamos el cluster (se demora un buen rato).
3. Configurar kubectl para que desde nuestro equipo (alienware) nos podamos comunicar con el cluster de kubernetes en aws.
Verificar que exista el archivo config en C:\Users\andre\.kube ; en este momento esta configurado para minikube, por lo que respaldamos antes de proseguir. Cuando ta esté lista la configuracion de kubectl para amazon este archivo va a cambiar automaticamente con los settings de minikube a aws.
Descargar la interfaz de comandos de aws para windows 64 (ver pantallazos). Instalar con archivo. exe. Verificar una vez q haya terminado en powershell como administrador con comando aws
A continuación debemos crear una clave de acceso (ver pantallazos) en nuestro perfil de usuarios en aws, que queda grabada en nuestro aws. Descargar este archivo, abrirlo y verificar que esten el AWSaccessKeyId y AWSsecretkeyId . 
Volver al powershell y escribir aws configure. Entonces pedirá el AWSaccessKeyId, por lo que se lo pasamos, el AWSsecretkeyId y la zona horaria us-east-1.
Luego, sobreescribir archivo config con comando aws eks --region us-east-1 update-kubeconfig --name k8s-curso, colver al archivo y verificar.
Verificar pods con comando kubectl get pods. Va a aparecer "No resources found in default namespace", lo q significa q estamos conectados ok, solo q aun no hay pods.
Con comando kubectl get services obtenemos el servicio por defecto de aws.
4. Crear los nodos en EC2 de AWS. En el paso 1 va a pedir Rol de IAM de nodo, y en este momento no tenemos ninguno, por lo q hay que crearlo.
Para ello hacer nos vamos a donde estan los roles y creamos un nuevo rol. En la siguiente pantalla seleccionamos EC2 y en la siguiente pantalla en "politicas de permiso" seleccionar eksworker y en la pantalla siguiente marcamos el ticket de AmazonEKSWorkerNodePolicy (no apretar "siguiente" en este caso).Luego en el buscador de "Politicas de permisos" buscamos cni y lo marcamos.
La última "Politica de permisos" a  marcar es ec2containerreg y luego marcamos AmazonEC2ContainerRegistryReadOnly.
Ahora si le damos click en siguiente, en pantalla siguiente le damos un nombre al rol y click en Crear Rol. Volveos a la pantalla de roles y vemos si esta el nuestro.
Volvemos a la pantalla de creacion de nodos, seleccionamos el rol q acabamos de crear y dejamos todo el resto de esta pantalla (pantalla ) por defecto.
Luego en el paso 2 ("Establecer la configuracion de informatica y de escalado", donde debemos especificar las propiedades de la maquina), seleccionar las propiedades adecuadas (ver pantallazos).
En paso 3 debemos especificar las redes. Por defecto ya dieran mostrarse seleccionadas las subnets que se generaron en el cluster. Lo dejamos tal cual.
En paso 4 revisamos y damos click en "crear". Entonces en la pantalla de "Grupos de nodos" va a aparecer "creando". Hay que estar actualizando hasta q ya haya sido creada y en la pantalla del cluster aparezcan los nodos (se demora!).Cuando ya este listo en el panel de EC2 van a aparecer dos instancias en ejecucion (creamos 2), pero si bien estan creadas hay q seguir esperando hasta q se inicializen (hay q ir actualizando la pagina).
5. Crear un sistema de almacenamiento con EFS (elastic file sistem) para guardar nuestas bbdd de postgress y mysql. Usaremos persistent volumes mediante el driver csi o Container Storage Interface ( a diferencia del local donde ocupamos hostPath). 
CSI es una interfaz q provee kubernetes para q los proveedores (en este caso aws) lo puedan implementar, en donde el sistema de almacenamiento esta 100% fuera de cualquier worker node. La implementacion de csi para el caso particular de aws es EFS, que por debajo es un NFS (net file system, o sea en la red). Para ello:
En buscador de aws escribimos efs y click en "crear sistema de archivos". Damos un nombre (para mysql en este caso), cambiamos el VPC al que creamos en pasos anteriores y click en "Personalizar".
El paso 1 lo dejamos tal cual. En paso 2 ("acceso a la red") Nos va a pedir grupos de seguridad, por lo q nos vamos a grupos de seguridad y creamos uno nuevo.
Para ello le damos un nombre al grupo, descripcion, seleccionamos el vpc de nuestro cluster. En "reglas de entrada" seleccionamos NFS en "tipo" y en origen debemos pegar el IPv4 de nuestro VPC. Luego click en "crear grupos de seguridad".
Volvemos al paso 2 y seleccionamos el grupo q acabamos de crear. El paso 3 lo dejamos tal cual y el 4 tambien. Hacemos click en "crear". Luego repitió lo mismo para postgres.
6. Configurar el driver CSI en el EFS. Para ello:
Copiar comando desde github (esta en los archivos adjuntos del curso, ver pantallazo). Pegar en el powershell abierto como administrador (le salió a el un error porwue no tenia git instalado, pero a mi no me debiera salir. Por lo mismo instaló git).
7. En nuestro proyecto modificar archivos persistent volume (ver archivo mysql-pv y mysql-pvc en ruta C:\Users\andre\PROCEDIMIENTOS\Kubernetes o C:\Users\andre\PROCEDIMIENTOS\Kubernetes\2_curso-kubernetes[secret]\curso-kubernetes). Hacemos lo mismo para ep pv y pvc de postgress. Ademas el deployment usuarios lo dejó con una réplica en vez de tres (o sea un pod en vez de tres). En este punto entonces tendrá dos nodos (creados en aws) con un pod (una replica )cada uno .
8. Desde la consola de nuestro proyecto modular en intellij, crear un role binding (ver comandos) para comunicar spring cloud con el cluster de kubernetes para q asi pueda extraer los pods y servicios (lo que interesa son las ips y los puertos de ellos) para q se puedan registrar en el servidor de nombre que se maneja por debajo en springcloud.
9. Desde la consola de nuestro proyecto modular en intellij, ejecutar cada uno de los yaml (excepto gateway) con comando apply.
10. svc-cursos.yaml y svc-usuarios.yaml se despliegan al final, y luego cuando le demos un comando kubectl get svc podremos ver que ya esta asociado a aws en forma automatica, ya q en EXTERNAL-IP muestra el load balancer de aws. Nos vamos al panel de EC2 en aws y van a aparecer todos los recursos q creamos (balanceadores de carga es lo mas importante y tienen el mismo nombre (que debiera ser igual a Nombre de DNS, ver pantallazo) q noa aparece en EXTERNAL-IP en nuestra consola local de intellij)
11. Probar en postman con dns:puerto (tuvo q crear usuarios y cursos en postman ya q obviamente por ser la primera vez la bbdd esta vacia en amazon).


SPRING SECURITY

1. Creó el proyecto msvc-auth en spring inizialzr con las dependencias spring security, spring web y spring reactive web, esta ultima dijo q era una alternativa a feignclient q es lo q se ocupa en los otros proyectos. En el pom del proyecto recien creado sin embargo, no le aparece una dependencia spring reactive web propiamente tal, sino spring-boot-starter-webflux
Agregó ademas las dependencias de kubernetes, agregó la version de spring cloud en el pom y lo dejó como modulo del proyecto parent curso-kubernetes. En la clase principal agregó @EnableDiscoveryClient.


2. Habilitó el proyecto msvc-auth como servidor de autorizacion.
Spring Authorization Server (documentacion) "SAS is a framework that provides implementation of the OAuth 2.1 and OpenIDConnect 1.0 specifications and other related specifications. It is built on top of
Spring Security to provide a secure, light-weight and customizable foundation for building OpenID Connect 1.0 Identity Providers and OAuth2 Authorization Server products. SAS can be used anywhere you already use Spring Security (o sea en el proyecto que acabamos de crear)" Es relativamente nuevo.
2.1.Agregó librería de oauth en el pom de msvc-auth, para q msvc-auth se convierta en un servidor de autorizacion para login
2.2.Copió y pegó directamente de la documentacion la clase SecurityConfig, q es la clase de configuracion de spring security (tuve algunos problemillas con letras rojas, por lo que mejor llegué y copié del codigo fuente del curso).

3. Habilitó nuestro servicio msvc-usuarios como cliente para el servidor de autorizacion msvc-auth, para poder hacer login (lo configuró para local/minikube primero, despues lo cambiará para kubernetes/aws). Para ello:
 3.1. Agregó la libreria de oauth2-client en el pom de msvc-usuarios.
 3.2. Creó un application.yml en msvc-usuarios y dentro configuró el cliente con la ruta de autorizacion
 3.3. En UsuarioController creó el metodo authorized para manejar la ruta dada en el paso 2, lo q devolverá clave-valor de codigo de autorizacion.
 3.4. Creó SecurityConfig en msvc-usuarios, que es la clase de configuracion de Spring Security para el cliente. En este caso msvc-usuarios es un cliente en el ambito de spring security


4. Configuró el servidor de recursos en msvc-usuarios, que es para proteger nuestras rutas. Para ello:
 1. Agregó libreria en el pom de msvc-usuarios
 2. Agregó configuracion en el application.yml
 3. En clase SecurityConfig de msvc-usuarios agregó antmatchers para dar permisos a las rutas
 
5.Configuró varibles de entorno en msvc-usuarios, para poder acceder desde el cliente (o sea msvc-usuarios) a los recursos del servidor de recuros en minikube. Para ello:
  5.1. Agregó variables de ambiente en application.yml ( ${LB_AUTH_ISSUER_URI:http://127.0.0.1:9000}). Ojo, que Andrés Guzman le llama loadBalancer a la IP.
  5.2. Agregó variables de ambiente Configmap
  5.3. Agregó variables de ambiente en deployment-usuarios
  5.4. Ya q anteriormente habia agregado librerias, volvio a construir la imagen para msvc-usuarios, darle un tag y subirla a dockerhub.
  
6.Configuro varibles de entorno en msvc-auth, para ello:
  6.1. Agregó las variables de entorno en metodo registeredClientRepository de SecurityConfig en msvc-auth.
  6.2. Copio y pego el dockerfile desde msvc-usuarios. Construyó la imagen, dio un tag y subió a dockerhub.
  
7. Escribio Deployment y Service de Auth
   7.1.Copió, pegó y modificó desde archivo gateway.yml el archivo auth.yml que va a servir a la vez a modo de deployment y servicio para msvc-auth. 
   7.2.Agregó variables de ambiente en auth.yml
   
8. Ejecutó las configuraciones de los pasos anteriores:
   8.1. Dió un comando apply a configmap.yml y a auth.yml para desplegar pods y generar servicios.
   8.2. Con comando minikube service msvc-usuarios --url obtuvo la ip y puerto de minikube (http://172.30.148.152:31492). Esta es la ruta del servidor de autorizacion.
   8.3.  Con comando minikube service msvc-auth --url obtuvo la ip y puerto de minikube (http://172.30.148.152:31326). Esta es la ruta para los clientes, que está protegida con el servidor de recursos y OAuthClient, que genera el token y autentica.
   8.4. En configmap pegó estas rutas en las variables de ambiente
   8.5. Pegó ambas rutas en el configmap y volvió a ejecutar el configmap.
   8.6. Borró el despliegue de msvc-usuarios.yaml y lo volvió a ejecutar (importante siempre levantar primero el servidor de autorizacion y luego los clientes, aunq en este caso por los pasos anteriores el servidor de autenticacion ya estaba levantado. Ademas siempre confirmar con los logs q despliegues esten bien).
   8.7. Probó en postman con las rutas de 8.2 y 8.3. Probó un litstar con un get y comprobó que ahora no lo puede ver debido a que en metodo securityFilterChain de SecurityConfig en msvc-usuarios se especifica q necesitamos autenticacion y la autorizacion "SCOPE_read","SCOPE_write"
   
9. Probó login mediante oauth2
   9.1.En barra de navegador escribió http://172.30.148.152:31326/oauth2/authorization/msvc-usuarios-client  , o sea ipminikube (obtenida en punto 8):puerto (tambien obtenido en punto 8)/oauth2/authorization/nombre del cliente escrito en configmap (msvc-usuarios-client). Entonces se le abre una pagina que dice Please Sign In.
   9.2. Escribió "admin" en user y 12345 en password, ya que eso esta en el SecurityConfig de msvc-auth. Se abre otra pagina, marcamos el ticket de consentimiento y enviamos.
   9.3. Entonces nos redirige al endpoint /authorized (q fue lo q seteamos en application.yml), con un codigo de autorizacion para iniciar sesion (que fue lo q seteamos en metodo authorized de UsuarioController). Copió el código para a continuacion probar en postman.
   9.4. Con la ruta del servidor de autorizacion de 8.2 probó en postman (ver los dos pantallazos q dice PruebaPostmanLoginOauth21). Enviamos y obtenemos el token en el body, q tiene una duracion de 5 minutos (se puede configurar en el servidor de autorizacion).
   9.5. Nota: ver pantallazos.Sin token primero, volver a probar el punto 8.7, va a fracasar. Luego, setear "bearer token" en postman, pegar token y volver a probar, entonces sí funciona el GET! 
   9.6. Probar post y put. veremos q aunque le pasomos el token igual no nos da acceso debido a los antmatchers del metodo securityFilterChain en SecuityConfig, ya que no tienen el "write" en este metodo.
   

10. Encriptó en usuarios controller, creó la imagen y subió. Luego hizo lo mismo del punto 9, para POST (ver pantallazo) para obtener el token y lo pegó para obtener la lista en el GET (igual q en punto 9).
Al igual q antes no le funciona el put (y esta bien). Para solucionar esto, en el applicationyml se podría agregar write en el scope. Sin embargo optó por probar en el navegador con la ruta especificada en pantallazo (ver pantallazo q dice urlParaWrite). 
Hizo lo mismo de antes y entonces sí pudo modificar con el put, obteniendo en el postman la contraseña encriptada (pantallazo PruebaPostmanConPasswordencriptadoEnPut)

        
11. Configuró BCrypt Password Encoder en el servidor de autorización
     11.1. Comentó las credenciales hardcodeadas en SecuityConfig del auth en el metodo UserDetailsService, reemplazandolo por el metodo configure. En este punto aun le queda implementar el UserDetailsService que escribió en este método
	 11.2. Comentó en metodo RegisteredClientRepository las lineas correspondiente a codigo hardcodeado, cambiandolas por las de código encriptado.

12. Configuró autenticacion por login por email.
     12.1. Escribió método handler login (UsuarioController).
	 12.2. En SecurityConfig de msvc-usuarios agregó "/login" en los antmatchers
	 12.3. En msvc-auth creó la clase WebClientConfig y escribió el método WebClient, que es la configuracion para poder usar webclient como balanceador de carga entre msvc-auth y msvc-usuarios (similar a feign).
	 12.4. Creó la clase UsuarioService en msvc-auth para hacer el login, con el metodo obligatorio loadUserByUserName en esta clase dando los authorities, igual que en el proyecto del mismo A.Guzman spring-boot-backend-apirest, excepto que en aquel proyecto el usuario que hay que procesar le llega desde el frontend y lo compara directamente con la bbdd.
	 En cambio en este proyecto en cambio le llega el usuario desde el cliente que es otro microservicio y mencionó (en realidad no mencionó, sino que puso un texto en el video, porque parece q se le olvidó) q por debajo spring security compara credenciales de forma automática.
	 12.5. Actualizó las imagenes tanto para usuarios como para auth, etiquetó y subió a dockerhub.
	 12.6. Setó replicas=0 (o sea eliminó los pods) en los deployment.yaml tanto para usuarios como para auth y las ejecutó con comando apply. Luego las cambió a replicas=1 y ejecutó auth y usuario (siempre el servidor de autorizacion primero).
	 12.7. Probó con la ruta de login (lo mismo que en el el punto 10, pero esta vez con login, ver pantallazos desde donde dice "captura de pantallas 139 en adelante" para esta parte (no confundir con las capturas de pantalla para el punto 10) para obtener permiso de lectura solamente (de hecho en la pantalla siguiente hay que marcar el checkbox de read). Obtuvo el code y con él generó el token. Pegó el token y con get vió que tenia acceso al listado pero no a ditar con put, asi que ejecutó la url con read y write (al igual q lo hizo en el punto 10), obtuvo código, token, pegó token en el put, ejecutó el put y vió que entonces sí podía  
	 12.8. Eliminó el puerto 8001 en metodo loadUserByUsername de UserService. Modificó clase WebClientConfig y volvió a probar todo de nuevo en postman para demostrar que el puerto no es necesario con esta modificación.
	 
13. Propagó el token de msvc-usuarios a msvc-cursos.
      13.1. Probó en postman un get con ipmsvc-curoso:puerto/4 (esto lo obtuvo con comando minikube service msvc-cursos --url). Vió q le falló porq no tiene el token.
 	  13.2. Modificó obtenerAlumnosPorCurso() de clase UsuarioClientRest en msvc-cursos para pasarle el token como Autorizacion en los headers en postman.
	  13.3. Modificó interfaz CursoService agregando el token en los argumentos que corresponden
	  13.4. Modificó CursoServiceImpl que implementa la clase CursoService para concordar con cambios en 13.3
	  13.5. Modificó detalle() en CursoController, para q pida el token como en 13.2.
	  13.6. Construyó imagenes, hizo tag y subió a dockerhub. Borró deployments y los ejecutó nuevamente.
	  13.7. Generó el token, lo pegó en Autorization bearer token de ipmsvc-curoso:puerto/4 y probó de nuevo. Esta vez sí le da un 200
	  
14. Login OAuth2 Cliente Http Postman. 
    Sucede que hasta el momento habia probado obtener el código de validacion previo al token mediante un formulario en el navegador. La idea es generarlo en postman para simular que este formulario esté en un cliente http como angular, react, etc.
	14.1. En  registeredClientRepository() de SecurityConfig de msvc-auth seteó en false el consentimiento y en defaultSecurityFilterChain() tambien de SecurityConfig msvc-auth deshabilitó csrf.
	14.2. Borró las imagenes, las etiquetó y las subió a dockerhub
	14.3. Borró los pods (comando apply deployment con replicas=0) y desplegó nuevamente para msvc-auth
	14.4. Se fue a Postman y con GET ipusuario:puertousuario/oauth2/authorization/msvc-usuarios-client (ver pantallazo "Captura de pantalla 156") obtuvo el formulario en el postman.
	14.5. (seguir viendo los pantallazos) De este punto en adelante es todo igual a antes en postman.
	
	
	 












