KUBERNETES

Importante: cosas como comentarios y explicaciones de archivos yaml estan en esta ruta: C:\Users\andre\PROCEDIMIENTOS\Kubernetes
Sin embargo, el proyecto como tal (codigo) se sigue modificando de lo q venia en Docker, o sea aca: C:\Users\andre\Desarrollo\Curso_Microservicios_Docker_Kubernates_AWS

Podemos darle instrucciones a nuestro Cluster por medio de un cliente externo que se está comunicando
El cliente envía instrucciones al cluster de kubernetes, el que por medio de la apiserver que se encuentra dentro del masternode se encarga de ejecutar adecuademente estas instrucciones, ya sea crear un pod, crear un deployment, etc.
El Scheduler, que se encuentra dentro del masternode, se encarga de ver la disponibilidad de recursos de las maquinas, asignando el recurso recien creado (pod, deployment, etc.) a la máquina mas liviana.
En el master node hay un componente llamado cloud controller manager manda una instruccion al cloud provider api (de AWS, por ejemplo) para q cree un load balancer.
Como clientes podemos comunicarnos con el Cluster de forma imperativa (con lineas de comandos de kubetcl en local) o bien declarativa por medio de un archivo yampl que contenga la configuracion de lo que deseamos hacer en el Cluster
El pod es un objeto de Kubernetes. Cada pod puede contener varios contenedores, pero lo usual es que sea solo uno. En el fondo es sólo una cáscara que envuelve al contenedor.
Los contenedores de un pod se comunican entre ellos por localhost.
Los pod se comunican entre si por IP adress, pero dado que los pods son altamente volátiles y desechables, no es recomendable unirlos de esta forma, sino por el IP estático de otro objeto como el service.
Los pods tb pueden tener BBDD por medio de volúmenes, pero ya q los pod son desechables, se recomienda guardar BBDD en el cluster.

El objeto deployment crea y administra los pods por nosotros. Es un archivo yaml con la configuracion de memoria, volumenes, etc. para nuestros pods. Lo mas importante es que contiene el estado deseado de despliegue para administrar los pods y los crea por nosotros.
El objeto service crea y administra una red por nosotros. Por medio del nombre o el IP de esta red se comunican los pods entre ellos.

minikube es una herramienta que permite virtualizar kubernetes, pero solo para trabajar en nuestro local.
kubectl es una herramienta que permite simular un cliente que está ejecutando kubernetes (en local kubectl se comunica con minukube).

Nota: Los comandos están en archivo "ComandosMiniKubeYKubectl".
Nota: Revisar los comentarios para los archivos yaml. Los comentarios están repartidos entre todos estos archivos, ya q se fue tomando nota a partir de lo que Guzman decía, por lo que se recomienda revisarlos todos

Lo que él hizo en local fue:
1.Levantó el cluster virtual con minikube.
2. Por medio del cliente kubectl creó imperativamente un deployment con la imagen mysql8 desde docker hub. Esto indirectamente generó ademas un pod. Notar que al parecer no existe un comando para crear pods directamente, sino q se hace a través de la creacion de un deployment.
3. En el paso anterior se dió cuenta de que no se puede agragar variables de ambiente por comandos, por lo que mejor generó un archivo yaml de deployment (ver comandos) y le agregó las variables de ambiente (clave valor para password y database). Luego ejecutó este archivo yaml para crear un deployment declarativamente. 
4. Creó un servicio al exponer el deployment de mysql8 (ver comandos), con el fin de obtener un nombre de dominio fijo y IP fija y de esta manera poder conectar los pod de mysql8 y msvc-usuarios, este último a ser creado aun.
5. Generó un deployment para msvc-usuarios, lo que indirectamente creó un pod. Lo hizo imperativamente, ya que acá sí tenía las variables de ambiente.
6. Expuso el deployment de msvc-usuarios, lo que indirectamente generó un servicio, pero no para comunicacion interna como en el caso anterior, sino para comunicación externa con kubectl expose deployment msvc-usuarios --port=8001 --type=LoadBalancer
7. Obtuvo la IP externa de msvc-usuarios en minikube (ver comandos) y probó en postman con esta ip y el puerto externo con GET http://ipMinikube:30507. Obtuvo un array de usuarios vacío, que debemos poblar mandando POST a http://ipMinikube:30507 con datos de usuario.
8. Actualizó la imagen de un deployment. 
Para ello en la clase UsuarioController, con el objetivo de generar un crash en la aplicación, agregó:
@GetMapping("/crash")
public void crash() {
((ConfigurableApplicationContext)context).close();
} 
A continuación construyó la imagen docker con este cambio y subió la imagen a docker hub.
Luego en el pod verificó el nombre del container que estaba usando y vió que no era la misma que acababa de subir a dockerhub. Por ello actualizó la imagen imperativamente (ver comandos).
Luego probó en postman con http:172.3187477474:30507/crash y vio que se le reinició la aplicacion debido al crash ya que con comando kubectl get pods aparece RESTARTS en 1 y no en 0. Ojo, en el pod en status debe aparecer como Running, no como Completed para q este levantado (se puede demorar un poco y aparecer como Completed primero).
Finalmente se creó una nueva imagen (o sea v3) eliminando los 20 seg en linea CMD del dockerfile de msvc-usuarios y volvió a revisar los pods. 
9. Escaló el deployment msvc-usuarios con tres replicas, con ello generamos tres instancias, o sea tres pods con carga balanceada.
10. Probó en postman con http:172.3187477474:30507/crash y con kubectl get pods vió que un pod tenía un restart de 1 , pero los otros 2 tienen restarts de 0 y status de Running, con lo q se comprueba que la carga está balanceada.
11. Volvió a escalar el deployment, pero solo con una réplica.
12. Generó un nuevo archivo yaml para el servicio de mysql8. (ver archivo de nombre svc-mysql.yaml) Este lo editó, ya q borró metadata como fecha de creación y otros datos. En spec solo dejó los puertos. Port es el puerto externo y targetPort es el interno del contenedor (para nuesto caso en ambos es 3306).
En este punto tiene dos archivos yaml: uno para el deployment llamado deployment-mysql.yaml y otro para el servicio llamado svc-mysql.yaml
13. Generó un nuevo archivo llamado svc-usuarios.yaml para msvc-usuarios (lo mismo del paso anterior pero para msvc-usuarios). ¿¿¿Anteriormente tenía el deployment-usuarios.yaml ???-->Al parecer, sí.
14. Generó un nuevo deployment a partir del archivo deployment-usuarios.yaml, con tres replicas. En este punto entonces tiene dos deployment: uno para mysql8 y otro para msvc-usuarios.
15. Verificó (con kubectl describe service msvc-usuarios) que en el selector aparece Selector: app=msvc-usuarios, ya q con esta etiqueta q escribimos en el yaml va a seleccionar los pods q contengan esa etiqueta, que en este caso fueron las tres réplicas o tres pods que creamos con el deployment de msvc-usuarios. Los endpoints de cada pod, con IP:puerto interno tambien lo podemos ver acá.
Estos pods son volátiles, por lo que mejor usamos la IP del servicio o el Name del servicio para comunicacion interna entre los pods.
16. Explicó que existe en minikube la interfaz gráfica dashboard. Esto es super util ya q ahorra tiempo. Se explica en el video titulado: Trabajando con la forma Declarativa yaml. Se activa con minikube dashboard.
17. Creó el archivo deployment-postgres.yaml (ver archivo) para hacer el deployment de postgres, lo q indirectamente generará un pod (solo uno, ya que no le puso replicas en el yaml)
18. Creó el archivo svc-postgres.yaml (ver archivo) para hacer el service de postgres.
19. Ejecutó ambos archivos anteriores(con comando apply, ver comandos). Luego de ejecutar tiene 5 pods es este punto: los tres de msvc-usuarios (creados con las replicas), uno de mysql y uno de postgress.
20. Escribió los archivos deployment-cursos.yaml y svc-cursos.yaml y los ejecutó (con comando apply). Obtuvo el puerto externo en forma análoga a como lo hizo para msvc-usuarios, pero esta vez para cursos (comando kubectl get services)
21. Para msvc-cursos obtvo la IP de minikube (comando minikube service msvc-cursos --url) y probó en postman con ruta GET Pminikube/puertoExterno
22. Luego de comprobar q funcionaba, en el mismo postman asignó usuarios a los cursos con la ruta PUT IPminikube/asignar-usuario/1 (1 porq es al curso numero 1)



VOLUMENES

Las BBDD si no se crean volúmenes  se encuentran en los containers. Sucede que muchas veces producto de trafico excesivo o de autoescalamiento el o los containers de un pod se reinician, perdiendose estos datos.
Es posible en Kubernetes tambien guardar volumenes en los pods (ver pdf de guzman), sin embargo si el pod se elimina tambien se pierde la informacion.
Es por ello que debemos crear volumenes que pertenezacan al worker node (o sae a la maquina) y no al container ni al pod, para persistir datos y que se puedan compartir siempre entre containers pertenecientes a la misma maquina.
Para ello requerimos un driver. Existen varios driver (driver=Types of Volumes) dependiendo del proveedor (aws, azure), etc, del numero de nodos y de otros factores. Uno de estos driver es CSI, que es una especificación que debe ser implementada por algun proveedor (como Hibernate implementa JPA) como AWS. CSI trabaja con varios nodos y por lo mismo suele ser ocupado en produccion
Para nuestro caso sin embargo que trabajamos com minikube con un solo nodo, usaremos el driver hostPath, q sirve para un nodo unico. 

Para la siguiente configuración, recordar q aunque el volumen está fuera del pod pero dentro del nodo debido al uso de hostPath, la configuración se hace en el deployment. 

1. Modificó el archivo deployment-mysql.yaml (ver archivo) incluyendo la configuracion de volumenes.
2. Con comando minikube service msvc-usuarios --url  y despues minikube service msvc-cursos --url verificó IP y puertos para probar en postman con ip:puerto con los GET y los POST
3. Ejecutó el deployment-mysql.yaml con comando apply (esto reinicia los containers, por lo que solo por esta vez el GET para usuarios estará vacío ). Para comprobarlo vió el log del pod con kubectl logs claveDelPod y se fijó q habia un error.
4. Con comando kubectl delete -f.\deployment-mysql.yaml borró el deployment (con los pods) debido a la situacion del punto anterior y luego lo creó de nuevo con comando apply. Comprobó q ahora si se estan guardando los datos aun si bota el pod debido al volumen q está fuera de este.
5. Realizó los mismos cuatro pasos anteriores pero para la BBDD de Postgres con el archivo deployment-postgres.yaml (no esta en el archivo mio porque es lo mismo y me dio lata, pero se puede descargar al final de la seccion)

Aparte de lo anterior existe la posibilidad de tener una configuración para volumenes que pertenezca al cluster y que no esté anclado a un despliegue en particular, por lo q el volumen no va a estar anclado a un pod o a un nodo. Esto se logra con
Persistant Volumes, en donde se tiene un archivo yaml con configuracion general, que podrá ser ocupado por uno o varios nodos o pods. En el yaml individual del pod debemos llamar a este archivo general a través de claims para tener acceso a los recursos guardados en el cluster.
Para pv:
1. Escribir el archivo de persistent volume (ver archivo mysql-pv.yaml).
2. Escribir el archivo de persistent volume claims para los pods (ver archivo mysql-pvc.yaml).
3. Modificar la configuracion de los pods incluyendo persistent volume claims en archivo deployment (ver archivo deployment-mysql.pvc.yaml).


CONFIGMAP Y SECRET

Nota:antes q nada en deployment-usuarios bajó el numero de replicas de tres a uno.
Configmap es una configuracion personalizada en donde todos los deployments (en este caso nuestros deployments para usuarios y cursos) estan en un mismo archivo. Para el configmap, los puertos host y database los saca de application.properties, como siempre
En los cuatro archivos yaml de deployments, debe hacerse referencia a las variables que seteamos en el configMap con valueFrom: y configMapKeyRef: (no lo puse en los archivos, pero se pueden ver al descargar los archivos del curso).
El archivo secret.yaml permite codificar en base 64 la informacion sensible, como username (root) y password (sasa). En los archivos de deployment debe hacerse referencia al secret.yaml por medio de valueFrom: secretKeyRef:
1. Escribió el configMap.
2. Modificó los cuatro deployments
3. Ejecutó el configMap y los 4 deployments (con comando appy).
4. Escribió el secret.yaml y en deployment-cursos y deployment-usuarios extrajo la info sensible desde el secret.
5. En el configmap comentó la info sensible, porque los deployments lo estan obteniendo del secret.
6. Desplegó todo de nuevo (incluyendo el secret, con comando apply como siempre) y volvio a probar en postman.

SPRING CLOUD KUBERNETES

Modificó los proyectos para q se pudieran comunicar mediante spring cloud kubernetes.
Para ello en ambos proyectos:
1. Agregó las tres dependencias de kubernetes.
2. Eliminó las url en las interfaz UsuarioClientRest y CursoClientRest, ya q en kubernetes la comunicacion es solo mediante nombre de servicio (que se encuentra en application.properties)
3. En ambas clases main agregó la configuracion @EnableDiscoveryClient //para poder usar spring cloud kubernetes
4. Generó las imagenes para ambos servicios en docker y las subió a docker hub.
5. Borró los deployment de msvc-cursos y msvc-usuarios, quedandose momentaneamente solo con los deployment de mysql y postgress.
6. Ejecutó los deployments borrados en el paso anterior (comando apply), para que tomara los cambios hechos a la imagen de dockerhub en pasos anteriores.
7. Pese a que los pods se muestran como running, con comando logs cachó que hay errores. Esto es porque los microservicios no se están comunicando al api de kubernetes. Para ello debemos dar privilegios de administrador (ver comandos).
8. Creó el priviliegio de administrador de cluster y ya los errores desaparecieron de los logs. Muestra un mensaje q la carga está balanceada para los pods via spring cloud debido a la configuracion q hicimos en el proyecto. Muestra tambien en los logs un mensaje de "discovery client is now available", lo q significa el microservicio (msvc-usuarios, por ejemplo)  se puede comunicar con los otros microservicios (msvc-cursos, por ejemplo). Muestra otro mensaje For 'msvc-usuarios' URL not provided. Will try picking an instance via load-balancing, o sea va a ocupar la configuracion de spring que realizamos en pasos anteriorespara balancear la carga si tuviera mas de una instancia.
9. Probó igual q anteriormente en postman para obtener cursoUsuarios con GET http://172.18.2013.250:32701 y vio q le funcionaba ok, pero esta vez es con la configuracion de spring cloud kubernetes q ocupa solo el nombre del servicio, prescindiendo de las url

Se pueden setear variables de ambiente para ver informacion en forma visual acerca del balanceo de carga entre los pods en el postman. Para ello, solo en msvc-usuarios realizó lo siguiente:
1. Agregó variables de ambiente en el archivo de deployment.yaml para ver la variable deseada segun la nomenclatura que esta estipulada en la documentacion de kubernetes. Para ver el nombre del pod e ip del pod (ver archivo q dice deployment-usuarios-seccion-spring.xml)
2. Incorporó estas variables en clase UsuarioController
3. Cambió el numero de réplicas a tres en msvc-usuarios. Desplegó nuevamente con comando apply
3. Probó en postman deshabilitando la opcion Connection en la cabecera. Probó varias veces con el get y vio que unas veces tomaba un nombre e ip de pod y otras veces cambiaba.

Tambien es posible setear las configuraciones del application properties fuera del proyecto de spring, en el configMap de Kubernetes, de esta forma si hay algun cambio de configuracion no hay q estar subiendo a dockerhub y haciendo todo el proceso de nuevo.
Para ello: 
1. Modificar el configMap.yaml agregando la configuracion application.yaml
2. Modificar UsuarioController (solo lo probó con msvc-usuarios) con clave valor a ser leído como texto en Postman
3. Notar que agregaríamos normalmente algo en application.properties para q el texto del punto anterior fuera leido; acá sin embargo no seteamos nada en el application.properties
4. Subir la imagen de nuevo a dockerhub. Borrar el deployment-usuarios y levantarlo de nuevo. Levantar tambien el configMap (comando apply)
5. Probar en postman con GET http://172.18.2013.250:32701

Las configuraciones del application properties en kubernetes de arriba pueden ser subdivididas en configuraciones para desarrollo y produccion. Para ello:
1. Setear en el application properties del proyecto (solo lo hizo con msvc-usuarios de nuevo) la configuracion para desarrollo o produccion con spring.profiles.active=dev o spring.profiles.active=prod (solo lo hizo con dev).
2. Escribir la configuracion en el configMap para dev y/o prod. No hizo nada en UsuarioController porque ya estaba agregado del exto anterior el put para que tomara cualquier texto q le pongamos en configMap.
3. Subir la imagen de nuevo a dockerhub. Borrar el deployment-usuarios y levantarlo de nuevo. Levantar tambien el configMap (comando apply)
4. Probar en postman con GET http://172.18.2013.250:32701